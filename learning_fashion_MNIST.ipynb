{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 从零开始实现softmax分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# noinspection PyTypeChecker\n",
    "from d2l import torch as d2l\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataloader_workers():\n",
    "    \"\"\"使⽤4个进程来读取数据\"\"\"\n",
    "    return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data_fashion_mnist(batch_size, resize=None):\n",
    "#     \"\"\"下载数据集并储存到内存中\"\"\"\n",
    "#\n",
    "#     trans = [transforms.ToTensor()]  # 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，\n",
    "#     if resize:\n",
    "#         \"\"\"\n",
    "#             resize可以将图像最小边缩放到指定值， 并且其他边同比例缩放\n",
    "#             example: torch.Size([32, 1, 32, 32])  -->  torch.Size([32, 1, 64, 64])\n",
    "#                      [1, 32, 32] --> [cross, height, weight]\n",
    "#         \"\"\"\n",
    "#         trans.insert(0, transforms.Resize(resize))\n",
    "#     trans = transforms.Compose(trans)\n",
    "#     mnist_train = torchvision.datasets.FashionMNIST(\n",
    "#     root='.data', train = True, transform=trans, download=True\n",
    "# )\n",
    "#     mnist_test = torchvision.datasets.FashionMNIST(\n",
    "#         root='.data', train = False, transform=trans, download=True\n",
    "#     )\n",
    "#     return  (data.DataLoader(mnist_train, batch_size, shuffle=True,                                                  num_workers=get_dataloader_workers()),\n",
    "#              data.DataLoader(mnist_test, batch_size, shuffle=True,\n",
    "#                              num_workers=get_dataloader_workers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train_iter, test_iter = load_data_fashion_mnist(32, resize=64)\n",
    "# for X, y in train_iter:\n",
    "#     print(X.shape, X.dtype, y.shape, y.dtype)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # softmax实现方式：\n",
    "    # 1. 对每个项求幂（使⽤exp）；\n",
    "    # 2. 对每⼀⾏求和（小批量中每个样本是⼀⾏），得到每个样本的规范化常数；\n",
    "    # 3. 将每⼀⾏除以其规范化常数，确保结果的和为1。\n",
    "    #\n",
    "    # tips: 因为one-hot编码是在最后一层做判断，所以要将这784个输入结果求和之后做softmax激活\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdim=True)  # 1表示按行求和, keepdim表示在列上不改变矩阵形状\n",
    "    return X_exp / partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 分析net的实现方式\n",
    "<img src=\"src/3.4.1_softmax.png\" width=400 height=200>\n",
    "\n",
    "============================\n",
    "\n",
    "如图，训练应该如下：\n",
    "o1 = x1w11 + x2w12 + x3w13 + x4w14 + b1,\n",
    "o2 = x1w21 + x2w22 + x3w23 + x4w24 + b2,\n",
    "o3 = x1w31 + x2w32 + x3w33 + x4w34 + b3.\n",
    "\n",
    "对于mnist训练集，输入层一共为784(x1, x2, x3,...,x784)，输出层一共为10(o1, o2, ... ,o10)\n",
    "那么每一个输出层都应该与相应的784个偏置项所对应\n",
    "所以W应该是[784, 10]的矩阵来和输入做乘法，之后再将结果输入到softmax激活函数中激活"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    # torch.matmul是一个线性函数，传入 W 和 b 后会根据广播机制实现 Y = WX + b\n",
    "    # 这里reshape可以将 X 按照 W 的维度展开，这样每个输入都会有一个 W\n",
    "    #\n",
    "    # X.size() = [batch_size, 1, 28, 28]\n",
    "    # X.reshape((-1, W.shape[0])) = [batch_size, 748]\n",
    "    # W.size() = [784, 10]\n",
    "    # torch.matmul(X.reshape((-1, W.shape[0])), W).size() = [256, 10]\n",
    "    #\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)\n",
    "\n",
    "def cross_entropy(y_hat, y):\n",
    "    # 交叉熵函数  -->  Cross Entropy.md\n",
    "    w = - torch.log(y_hat[range(len(y_hat)), y])\n",
    "    return w\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "def evaluate_accuracy(net, data_iter):\n",
    "    \"\"\"计算模型在指定数据集上的精度\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # 将模型设置为评估模式\n",
    "    metric = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), y.numel())  # numel()返回该张量总元素的个数\n",
    "    return metric[0] / metric[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def train_epoch_ch3(net, train_iter, loss, updater):\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        # isinstance(A， B)  -->  A是要判断的对象, B是要判断的类型, 返回布尔值\n",
    "        net.train()  # train model\n",
    "\n",
    "    # 训练损失总和、训练准确度总和、样本数\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # 计算梯度并更新参数\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)  # 传入自定义损失函数\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            # 使用pytorch内置的优化器和算是函数\n",
    "            updater.zero_grad()\n",
    "            l.sum().backward()\n",
    "            updater.step()\n",
    "        else:\n",
    "            # 使用定制的优化器和损失函数\n",
    "            # backward()所接受传入的必须是标量, backward()接受传入的有一个参数grad_tensors, 这个参数是指定哪些方向上的向量需要反向传播\n",
    "            # 若传入torch.ones(len(X)), 则表示各个方向均需要反向传播, 因为 1 作为常数对反向传播无影响, 若传入 0 则表示该方向不传播\n",
    "            # backward(torch.ones(len(X))) 是等效于 X.sum().backward() 因为求和操作就相当于X点乘一个全是 1 的矩阵\n",
    "            l.sum().backward()\n",
    "            print(X.size())\n",
    "            updater(X.shape[0])\n",
    "        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "\n",
    "    # 返回训练损失和训练精度\n",
    "    return metric[0] / metric[2], metric[1]/ metric[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Animator:  #@save\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: d2l.set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# noinspection PyShadowingNames\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch+1, train_metrics+ (test_acc, ))\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert 1 >= train_acc > 0.7, train_acc\n",
    "    assert 1 >= test_acc > 0.7, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "def updater(batch_size):\n",
    "    # 在sgd函数中, with torch.no_grad() 在该模块下计算出来的tensor的required_grad 设置为False, 这样不会自动求导, 节约内存\n",
    "    return d2l.sgd([W, b], lr, batch_size)  # sgd 是自动随机梯度下降\n",
    "\n",
    "num_epochs = 100\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### softmax简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))  # 用 Flatten 展平层展开输入\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        #  输入一个线性连接层, 然后用_normal方法，以正态分布初始化线性连接层的w\n",
    "        #  m.weight  --> W  |  m.bias  --> b\n",
    "        #  nn.init.normal_() --> 正态分布初始化  |  nn.init.zeros_()  --> 初始化成0\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights)  # apply()类似于一个for循环, 将fn应用到每一个model中, fn的输入即为每个models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "num_epochs = 10\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd034ed54560f0698c2946b7ca675e493afbd7ee3c0ecf162ae3deac3cf4477b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}